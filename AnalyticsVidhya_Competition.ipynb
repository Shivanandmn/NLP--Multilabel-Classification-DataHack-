{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "analytics_independence_hackthon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnYueDJwrIQzNSkie6aNsq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEZEmGV9tjVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q transformers "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9CzeYBK8FUs",
        "colab_type": "text"
      },
      "source": [
        "Here we import everything required for preprocessing, training and evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XasnHLwVuNdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm \n",
        "from sklearn import model_selection,metrics\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MkrYpM48nXb",
        "colab_type": "text"
      },
      "source": [
        "**Preprocessing**\\\n",
        "Stripping extra whitespaces around the text.\n",
        "Replacing escape characters with whitespace.\n",
        "Padding all punctuations with whitespaces on both sides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp2N-CTr8ki8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(dataframe):  \n",
        "  def clean_abstract(text):\n",
        "    text = text.split()\n",
        "    text = [x.strip() for x in text]\n",
        "    text = [x.replace('\\n',' ').replace(\"\\t\",\" \") for x in text]\n",
        "    text = \" \".join(text)\n",
        "    text = re.sub(\"([.,?()])\",r\"\\1\",text)\n",
        "    return text\n",
        "  \n",
        "  def get_texts(dataframe):\n",
        "    dataframe[\"ABSTRACT\"] = dataframe[\"ABSTRACT\"].apply(clean_abstract)\n",
        "    return dataframe.iloc[:,1:3]\n",
        "\n",
        "  def get_labels(dataframe):\n",
        "    label_df = dataframe.iloc[:,3:]\n",
        "    return label_df\n",
        "  return get_texts(dataframe),get_labels(dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5T77ds_9OeX",
        "colab_type": "text"
      },
      "source": [
        "**Dataset preparation** \\\n",
        "Getting text and tokenizing with scientific-bert tokenizer from bert based models of Hugging-face transformers. Tokenizer returns input ids and attention masks, this helpful for supplying directly to its model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfbW0FLC8gKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,title,abstract,targets,tokenizer,max_len):\n",
        "    super(BERTDataset,self).__init__()\n",
        "    self.title = title\n",
        "    self.abstract = abstract\n",
        "    self.targets = targets\n",
        "    self.tokenize = tokenizer\n",
        "    self.max_len = max_len\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.title)\n",
        "  \n",
        "  def __getitem__(self,item):\n",
        "    title = str(self.title[item])\n",
        "    abstract = str(self.abstract[item])\n",
        "    #merging title and abstract as one text \n",
        "    input = self.tokenize.encode_plus(\n",
        "        title +' '+ abstract,\n",
        "        None,\n",
        "        truncation = True,\n",
        "        return_token_type_ids = False,\n",
        "        max_length = self.max_len,\n",
        "        padding='max_length',\n",
        "    )\n",
        "    ids = input['input_ids']\n",
        "    mask = input['attention_mask'] \n",
        "    return {\n",
        "        'ids':torch.tensor(ids, dtype = torch.long),\n",
        "        'mask': torch.tensor(mask,dtype = torch.long),\n",
        "        'targets': torch.tensor(self.targets[item,:], dtype = torch.float)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiWbTcgB-gvw",
        "colab_type": "text"
      },
      "source": [
        "**Model** \\\n",
        "Here we use transformer model and fully connected linear layer to get the required number of ouput labels. As usual transformer model outputs 768.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4wmE4q09M5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTBaseUncased(nn.Module):\n",
        "    def __init__(self,bert_pretrained_weights):\n",
        "      super(BERTBaseUncased,self).__init__()\n",
        "      bert_model = bert_pretrained_weights\n",
        "      self.bert = transformers.AutoModel.from_pretrained(bert_model)\n",
        "      self.bert_drop = nn.Dropout(0.3)\n",
        "      self.fc = nn.Linear(768,NUM_LABELS)\n",
        "    \n",
        "    def forward(self,ids,masks):\n",
        "      _,o2 = self.bert(\n",
        "          input_ids = ids,\n",
        "          attention_mask = masks,\n",
        "          #token_type_ids = token_type_ids        \n",
        "          ) \n",
        "      o = self.bert_drop(o2)\n",
        "      return self.fc(o)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEezqOY_ARUv",
        "colab_type": "text"
      },
      "source": [
        "**Loss Function**\n",
        "BCEWithLogitsLoss is most choosen for use, because it comes with sigmoid layer included with bce logloss, and greater stability. \\\n",
        "*BCEWithLogitsLoss = Sigmoid Layer + BCELoss*\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Good thing about BCEWithLogitsloss is its weight components in the equation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IOScSTYuT2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(output,targets):\n",
        "  return nn.BCEWithLogitsLoss()(output,targets)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnrLbC1LC4j6",
        "colab_type": "text"
      },
      "source": [
        "**Training function and Evaluation function** \\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rL71jMICzyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_fn(data_loader,model,device,optimizer,scheduler=None):\n",
        "  model.train()\n",
        "  avg_loss=0\n",
        "  for b,d in tqdm(enumerate(data_loader),total = len(data_loader)):\n",
        "    ids = d['ids']\n",
        "    mask = d['mask']\n",
        "    targets = d['targets']\n",
        "    ids = ids.to(device, dtype = torch.long)\n",
        "    mask = mask.to(device, dtype = torch.long)\n",
        "    targets = targets.to(device, dtype = torch.float)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(ids = ids,\n",
        "                   masks = mask,\n",
        "                   )\n",
        "    loss = loss_fn(output,targets)\n",
        "    loss.backward()\n",
        "    avg_loss+=loss\n",
        "    optimizer.step()\n",
        "    if scheduler is not None:\n",
        "      scheduler.step()\n",
        "    \n",
        "  return avg_loss/len(data_loader) \n",
        " \n",
        " \n",
        " \n",
        "def eval_fn(data_loader,model,device):\n",
        "  model.eval()\n",
        "  fin_output = []\n",
        "  fin_target = []\n",
        "  fin_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for b,d in tqdm(enumerate(data_loader),total = len(data_loader)):\n",
        "        ids = d['ids']\n",
        "        mask = d['mask']\n",
        "        targets = d['targets']\n",
        "        ids = ids.to(device, dtype = torch.long)\n",
        "        mask = mask.to(device, dtype = torch.long)\n",
        "        targets = targets.to(device, dtype = torch.float)\n",
        "        output = model(ids = ids,\n",
        "                      masks = mask,\n",
        "                      )\n",
        "        loss = loss_fn(output,targets)\n",
        "        logits = torch.sigmoid(output)\n",
        "        output = np.round(logits.cpu().detach().numpy())\n",
        "        fin_output.extend(output)\n",
        "        fin_target.extend(targets.cpu().detach().numpy())\n",
        "        fin_loss+=loss\n",
        "  return loss/len(data_loader),fin_output,fin_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DfAhzOwDZXf",
        "colab_type": "text"
      },
      "source": [
        "**Training the model** \\\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A59rzKULkpt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def run():\n",
        "  dfx = pd.read_csv('/content/train.csv').fillna(\"none\")\n",
        "  sample = pd.read_csv(\"sample_submission.csv\")\n",
        "  test_cols = list(sample.drop(columns = \"ID\").columns)\n",
        "  df_train,df_valid = model_selection.train_test_split(dfx,\n",
        "                                                       test_size = 0.25,\n",
        "                                                       random_state = 42)\n",
        "  df_train = df_train.reset_index(drop=True)\n",
        "  df_valid = df_valid.reset_index(drop=True)\n",
        "  train_targets = df_train[test_cols].values\n",
        "  valid_targets = df_valid[test_cols].values\n",
        "  train_dataset = BERTDataset(title=df_train.TITLE.values,\n",
        "                              abstract = df_train.ABSTRACT.values,\n",
        "                              targets = train_targets,\n",
        "                              tokenizer = TOKENIZER,\n",
        "                              max_len = MAX_LEN)\n",
        "  \n",
        "  train_dataloader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size = TRAIN_BATCH_SIZE,\n",
        "      shuffle = True,\n",
        "      num_workers = 0\n",
        "      )\n",
        "  valid_dataset = BERTDataset(title=df_valid.TITLE.values,\n",
        "                              abstract = df_valid.ABSTRACT.values,\n",
        "                              targets = valid_targets,\n",
        "                              tokenizer = TOKENIZER,\n",
        "                              max_len = MAX_LEN)\n",
        "  valid_dataloader = torch.utils.data.DataLoader(\n",
        "      valid_dataset,\n",
        "      batch_size = VALID_BATCH_SIZE,\n",
        "      shuffle = False,\n",
        "      num_workers = 0\n",
        "      ) \n",
        "  model = BERTBaseUncased(MODAL_PATH).to(device)\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "  optimizer_parameters = [\n",
        "      {\n",
        "          \"params\": [\n",
        "              p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "          ],\n",
        "          \"weight_decay\": 0.001,\n",
        "      },\n",
        "      {\n",
        "          \"params\": [\n",
        "              p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "          ],\n",
        "          \"weight_decay\": 0.01,\n",
        "      },\n",
        "  ]\n",
        "  optimizer = transformers.AdamW(optimizer_parameters, lr=LR)\n",
        "  num_training_steps = len(train_dataloader) * EPOCHS\n",
        "  scheduler = get_linear_schedule_with_warmup(\n",
        "      optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=num_training_steps\n",
        "  )\n",
        "\n",
        "  max_f1_score = 0\n",
        "  for epoch in range(EPOCHS):\n",
        "    train_fn(\n",
        "        data_loader = train_dataloader,\n",
        "        model = model,\n",
        "        device = device,\n",
        "        optimizer = optimizer)\n",
        "    loss, output, target = eval_fn(\n",
        "        data_loader = valid_dataloader,\n",
        "        model =model,\n",
        "        device = device)\n",
        "    f1_score = metrics.f1_score(target,output,average = 'micro')\n",
        "    if f1_score > max_f1_score:\n",
        "      torch.save(model.state_dict(),'model.bin')\n",
        "      print(f\"model Saved {f1_score}\")\n",
        "      max_f1_score = f1_score\n",
        "    print(f\"EPOCH: {epoch+1}, microF1_SCORE : {f1_score}\")\n",
        "  return f1_score "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuQ2lniYEyAs",
        "colab_type": "text"
      },
      "source": [
        "CONFIGURATION DATA FOR TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRBEmK7Gxl3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "09beb460-664e-4e13-9ecf-9909cef20e7d"
      },
      "source": [
        "MAX_LEN = 320\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 8\n",
        "VALIDATION_SPLIT = 0.25\n",
        "MODAL_PATH  = \"allenai/scibert_scivocab_uncased\"\n",
        "TOKENIZER = transformers.AutoTokenizer.from_pretrained(MODAL_PATH)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LR = 3e-5\n",
        "EPOCHS = 5\n",
        "SEED = 32\n",
        "NUM_LABELS = 6\n",
        "\n",
        "#Training starts here\n",
        "run() "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 984/984 [28:34<00:00,  1.74s/it]\n",
            "100%|██████████| 656/656 [03:48<00:00,  2.87it/s]\n",
            "  0%|          | 0/984 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model Saved 0.8349499495615738\n",
            "EPOCH: 1, microF1_SCORE : 0.8349499495615738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 984/984 [28:37<00:00,  1.74s/it]\n",
            "100%|██████████| 656/656 [03:48<00:00,  2.87it/s]\n",
            "  0%|          | 0/984 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model Saved 0.8425339366515836\n",
            "EPOCH: 2, microF1_SCORE : 0.8425339366515836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 984/984 [28:36<00:00,  1.74s/it]\n",
            "100%|██████████| 656/656 [03:48<00:00,  2.88it/s]\n",
            "  0%|          | 0/984 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 3, microF1_SCORE : 0.8337389717067235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 984/984 [28:36<00:00,  1.74s/it]\n",
            "100%|██████████| 656/656 [03:48<00:00,  2.87it/s]\n",
            "  0%|          | 0/984 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 4, microF1_SCORE : 0.8386813186813187\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 984/984 [28:37<00:00,  1.75s/it]\n",
            "100%|██████████| 656/656 [03:48<00:00,  2.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 5, microF1_SCORE : 0.8340515612005746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8340515612005746"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bKAggc0VYEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfrtaC8aVZK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}